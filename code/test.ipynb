{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cyn/anaconda3/envs/cyn1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import timm\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transform\n",
    "from sklearn import model_selection\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from timm.models import create_model\n",
    "\n",
    "from dataset import CustomDataset, MultiviewImgDataset, SingleimgDataset\n",
    "from sklearn.metrics import classification_report\n",
    "import models.create_models as create\n",
    "# from ml_decoder import MLDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PATH = \"/disk1/chenyangneng/data/MFIDDR\"\n",
    "# TEST_PATH = \"/disk1/chenyangneng/data/MFIDDR/test/\"\n",
    "# test_csv_path = os.path.join(DATA_PATH, 'test_fourpic_label.csv')\n",
    "# test_df = pd.read_csv(test_csv_path)\n",
    "# # 创建数据集实例\n",
    "# test_dataset = MultiviewImgDataset(TEST_PATH, test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn.modules.transformer import _get_activation_fn\n",
    "\n",
    "\n",
    "def add_ml_decoder_head(\n",
    "    model,\n",
    "    num_classes=-1,\n",
    "    num_of_groups=-1,\n",
    "    decoder_embedding=768,\n",
    "    zsl=0,\n",
    "    num_layers_decoder=1,  # 修改\n",
    "):\n",
    "    if num_classes == -1:\n",
    "        num_classes = model.num_classes\n",
    "    num_features = model.num_features\n",
    "    if hasattr(model, \"global_pool\") and hasattr(model, \"fc\"):  # resnet50\n",
    "        model.global_pool = nn.Identity()\n",
    "        del model.fc\n",
    "        model.fc = MLDecoder(\n",
    "            num_classes=num_classes,\n",
    "            initial_num_features=num_features,\n",
    "            num_of_groups=num_of_groups,\n",
    "            decoder_embedding=decoder_embedding,\n",
    "            zsl=zsl,\n",
    "            num_layers_decoder=num_layers_decoder,\n",
    "        )\n",
    "    elif hasattr(model, \"head\"):  # tresnet\n",
    "        if hasattr(model, \"global_pool\"):\n",
    "            model.global_pool = nn.Identity()\n",
    "        del model.head\n",
    "        model.head = MLDecoder(\n",
    "            num_classes=num_classes,\n",
    "            initial_num_features=num_features,\n",
    "            num_of_groups=num_of_groups,\n",
    "            decoder_embedding=decoder_embedding,\n",
    "            zsl=zsl,\n",
    "        )\n",
    "    else:\n",
    "        print(\"model is not suited for ml-decoder\")\n",
    "        exit(-1)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "class TransformerDecoderLayerOptimal(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        nhead=8,\n",
    "        dim_feedforward=2048,\n",
    "        dropout=0.1,\n",
    "        activation=\"relu\",\n",
    "        layer_norm_eps=1e-5,\n",
    "    ) -> None:\n",
    "        super(TransformerDecoderLayerOptimal, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.norm3 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if \"activation\" not in state:\n",
    "            state[\"activation\"] = torch.nn.functional.relu\n",
    "        super(TransformerDecoderLayerOptimal, self).__setstate__(state)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt: Tensor,\n",
    "        memory: Tensor,\n",
    "        tgt_mask: Optional[Tensor] = None,\n",
    "        memory_mask: Optional[Tensor] = None,\n",
    "        tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "        memory_key_padding_mask: Optional[Tensor] = None,\n",
    "    ) -> Tensor:\n",
    "        tgt = tgt + self.dropout1(tgt)\n",
    "        tgt = self.norm1(tgt)\n",
    "        print(\"-----------\",tgt.shape,memory.shape)\n",
    "        tgt2 = self.multihead_attn(tgt, memory, memory)[0]\n",
    "        print(tgt2.shape)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "\n",
    "# @torch.jit.script\n",
    "# class ExtrapClasses(object):\n",
    "#     def __init__(self, num_queries: int, group_size: int):\n",
    "#         self.num_queries = num_queries\n",
    "#         self.group_size = group_size\n",
    "#\n",
    "#     def __call__(self, h: torch.Tensor, class_embed_w: torch.Tensor, class_embed_b: torch.Tensor, out_extrap:\n",
    "#     torch.Tensor):\n",
    "#         # h = h.unsqueeze(-1).expand(-1, -1, -1, self.group_size)\n",
    "#         h = h[..., None].repeat(1, 1, 1, self.group_size) # torch.Size([bs, 5, 768, groups])\n",
    "#         w = class_embed_w.view((self.num_queries, h.shape[2], self.group_size))\n",
    "#         out = (h * w).sum(dim=2) + class_embed_b\n",
    "#         out = out.view((h.shape[0], self.group_size * self.num_queries))\n",
    "#         return out\n",
    "\n",
    "\n",
    "# @torch.jit.script\n",
    "class GroupFC(object):\n",
    "    def __init__(self, embed_len_decoder: int):\n",
    "        self.embed_len_decoder = embed_len_decoder\n",
    "\n",
    "    def __call__(\n",
    "        self, h: torch.Tensor, duplicate_pooling: torch.Tensor, out_extrap: torch.Tensor\n",
    "    ):\n",
    "        for i in range(h.shape[1]):\n",
    "            h_i = h[:, i, :]\n",
    "            if len(duplicate_pooling.shape) == 3:\n",
    "                w_i = duplicate_pooling[i, :, :]\n",
    "            else:\n",
    "                w_i = duplicate_pooling\n",
    "            out_extrap[:, i, :] = torch.matmul(h_i, w_i)\n",
    "\n",
    "\n",
    "class MLDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        num_of_groups=-1,\n",
    "        decoder_embedding=768,# 3072\n",
    "        initial_num_features=2048,# 3072\n",
    "        zsl=0,\n",
    "        num_layers_decoder=1,  # 修改\n",
    "    ):\n",
    "        super(MLDecoder, self).__init__()\n",
    "        embed_len_decoder = 100 if num_of_groups < 0 else num_of_groups\n",
    "        if embed_len_decoder > num_classes:\n",
    "            embed_len_decoder = num_classes\n",
    "\n",
    "        # switching to 768 initial embeddings\n",
    "        decoder_embedding = 768 if decoder_embedding < 0 else decoder_embedding\n",
    "        embed_standart = nn.Linear(initial_num_features, decoder_embedding)\n",
    "\n",
    "        # non-learnable queries\n",
    "        if not zsl:\n",
    "            query_embed = nn.Embedding(embed_len_decoder, decoder_embedding)\n",
    "            query_embed.requires_grad_(False)\n",
    "        else:\n",
    "            query_embed = None\n",
    "\n",
    "        # decoder\n",
    "        decoder_dropout = 0.1\n",
    "        # num_layers_decoder = 5\n",
    "        dim_feedforward = 2048\n",
    "        layer_decode = TransformerDecoderLayerOptimal(\n",
    "            d_model=decoder_embedding,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=decoder_dropout,\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            layer_decode, num_layers=num_layers_decoder\n",
    "        )\n",
    "        # self.decoder = layer_decode  # 修改\n",
    "        self.decoder.embed_standart = embed_standart\n",
    "        self.decoder.query_embed = query_embed\n",
    "        self.zsl = zsl\n",
    "\n",
    "        if self.zsl:\n",
    "            if decoder_embedding != 300:\n",
    "                self.wordvec_proj = nn.Linear(300, decoder_embedding)\n",
    "            else:\n",
    "                self.wordvec_proj = nn.Identity()\n",
    "            self.decoder.duplicate_pooling = torch.nn.Parameter(\n",
    "                torch.Tensor(decoder_embedding, 1)\n",
    "            )\n",
    "            self.decoder.duplicate_pooling_bias = torch.nn.Parameter(torch.Tensor(1))\n",
    "            self.decoder.duplicate_factor = 1\n",
    "        else:\n",
    "            # group fully-connected\n",
    "            self.decoder.num_classes = num_classes\n",
    "            self.decoder.duplicate_factor = int(num_classes / embed_len_decoder + 0.999)\n",
    "            self.decoder.duplicate_pooling = torch.nn.Parameter(\n",
    "                torch.Tensor(\n",
    "                    embed_len_decoder, decoder_embedding, self.decoder.duplicate_factor\n",
    "                )\n",
    "            )\n",
    "            self.decoder.duplicate_pooling_bias = torch.nn.Parameter(\n",
    "                torch.Tensor(num_classes)\n",
    "            )\n",
    "        torch.nn.init.xavier_normal_(self.decoder.duplicate_pooling)\n",
    "        torch.nn.init.constant_(self.decoder.duplicate_pooling_bias, 0)\n",
    "        self.decoder.group_fc = GroupFC(embed_len_decoder)\n",
    "        self.train_wordvecs = None\n",
    "        self.test_wordvecs = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 4:  # [bs,2048, 7,7] [bs,3072,17,17]\n",
    "            embedding_spatial = x.flatten(2).transpose(1, 2) #  [bs,289,3072]\n",
    "        else:  # [bs, 197,468]\n",
    "            embedding_spatial = x\n",
    "        embedding_spatial_786 = self.decoder.embed_standart(embedding_spatial)# linear(3072,3072)\n",
    "        embedding_spatial_786 = torch.nn.functional.relu(\n",
    "            embedding_spatial_786, inplace=True\n",
    "        )\n",
    "\n",
    "        bs = embedding_spatial_786.shape[0]\n",
    "        if self.zsl:\n",
    "            query_embed = torch.nn.functional.relu(\n",
    "                self.wordvec_proj(self.decoder.query_embed)\n",
    "            )\n",
    "        else:\n",
    "            query_embed = self.decoder.query_embed.weight\n",
    "        # tgt = query_embed.unsqueeze(1).repeat(1, bs, 1)\n",
    "        # query_embed [100,3072]\n",
    "        tgt = query_embed.unsqueeze(1).expand(\n",
    "            -1, bs, -1\n",
    "        )  # no allocation of memory with expand \n",
    "        # tgt [100,bs,3072]\n",
    "        # embedding_spatial_786 [bs,289,3072]\n",
    "        print(\"tgt\",tgt.shape)\n",
    "        print(\"embedding_spatial_786\",embedding_spatial_786.shape)\n",
    "        h = self.decoder(\n",
    "            tgt, embedding_spatial_786.transpose(0, 1)\n",
    "        )  # [embed_len_decoder, batch, 768]\n",
    "        h = h.transpose(0, 1)\n",
    "        print(\"h\",h.shape)\n",
    "        out_extrap = torch.zeros(\n",
    "            h.shape[0],\n",
    "            h.shape[1],\n",
    "            self.decoder.duplicate_factor,\n",
    "            device=h.device,\n",
    "            dtype=h.dtype,\n",
    "        )\n",
    "        self.decoder.group_fc(h, self.decoder.duplicate_pooling, out_extrap)\n",
    "        print(\"s-----\",out_extrap.shape)\n",
    "        if not self.zsl:\n",
    "            h_out = out_extrap.flatten(1)[:, : self.decoder.num_classes]\n",
    "        else:\n",
    "            h_out = out_extrap.flatten(1)\n",
    "        h_out += self.decoder.duplicate_pooling_bias\n",
    "        logits = h_out\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "head=MLDecoder(num_classes=5,decoder_embedding=3072,initial_num_features=3072,num_layers_decoder=8,num_of_groups=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=torch.zeros(8,3072,17,17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tgt torch.Size([5, 8, 3072])\n",
      "embedding_spatial_786 torch.Size([8, 289, 3072])\n",
      "----------- torch.Size([5, 8, 3072]) torch.Size([289, 8, 3072])\n",
      "torch.Size([5, 8, 3072])\n",
      "----------- torch.Size([5, 8, 3072]) torch.Size([289, 8, 3072])\n",
      "torch.Size([5, 8, 3072])\n",
      "----------- torch.Size([5, 8, 3072]) torch.Size([289, 8, 3072])\n",
      "torch.Size([5, 8, 3072])\n",
      "----------- torch.Size([5, 8, 3072]) torch.Size([289, 8, 3072])\n",
      "torch.Size([5, 8, 3072])\n",
      "----------- torch.Size([5, 8, 3072]) torch.Size([289, 8, 3072])\n",
      "torch.Size([5, 8, 3072])\n",
      "----------- torch.Size([5, 8, 3072]) torch.Size([289, 8, 3072])\n",
      "torch.Size([5, 8, 3072])\n",
      "----------- torch.Size([5, 8, 3072]) torch.Size([289, 8, 3072])\n",
      "torch.Size([5, 8, 3072])\n",
      "----------- torch.Size([5, 8, 3072]) torch.Size([289, 8, 3072])\n",
      "torch.Size([5, 8, 3072])\n",
      "h torch.Size([8, 5, 3072])\n",
      "s----- torch.Size([8, 5, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1128, -1.6580,  1.3629,  0.6822,  0.1324],\n",
       "        [-1.6582, -1.4632,  0.4433,  1.6761, -0.6224],\n",
       "        [-2.0831, -0.9010,  1.5078, -0.1696,  0.0536],\n",
       "        [-0.8993, -1.1478,  1.3426,  1.3557, -0.0658],\n",
       "        [-1.7047,  0.1282,  1.8003,  0.8549,  0.5491],\n",
       "        [-1.9468, -0.6957,  0.5339,  1.3930, -0.0485],\n",
       "        [-0.5730, -0.5004,  2.5687,  1.4899,  0.1522],\n",
       "        [-1.7733, -1.3661,  1.7731,  1.8172,  0.5989]],\n",
       "       grad_fn=<AsStridedBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head(h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ret",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
